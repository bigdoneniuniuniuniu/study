# Apache Hadoop Learning
[TOC]

## HDFS
### ç®€ä»‹
```
HDFSç§°ä¸ºåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆHadoop Distributed Filesystemï¼‰ï¼Œæœ‰æ—¶ä¹Ÿç®€ç§°ä¸ºDFSã€‚
æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹å‡ ä¸ªkeyæè¿°HDFSï¼š
	- è¶…å¤§æ–‡ä»¶
		GBã€TBç”šè‡³PBçº§åˆ«çš„æ•°æ®ã€‚
	- æµå¼æ•°æ®è®¿é—®
		æ•°æ®é›†é€šå¸¸ç”±æ•°æ®æºç”Ÿæˆæˆ–ä»æ•°æ®æºå¤åˆ¶è€Œæ¥çš„ï¼Œç„¶ååœ¨é•¿æ—¶é—´åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œå„ç§æ•°æ®åˆ†æã€‚å› æ­¤â€œä¸€æ¬¡å†™å…¥ï¼Œå¤šæ¬¡è¯»å–â€æ˜¯æœ€é«˜æ•ˆçš„è®¿é—®æ¨¡å¼ã€‚
	- è¦æ±‚ä½æ—¶é—´å»¶è¿Ÿæ•°æ®è®¿é—®çš„åº”ç”¨ï¼Œä¸é€‚åˆåœ¨HDFSä¸Šè¿è¡Œ
	- å¤§é‡çš„å°æ–‡ä»¶
	  namenodeå°†æ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œå› æ­¤æ–‡ä»¶ç³»ç»Ÿæ‰€èƒ½å­˜å‚¨çš„æ–‡ä»¶æ€»æ•°å—é™äºnamenodeçš„å†…å­˜å®¹é‡ã€‚
	- æ–‡ä»¶å†™å…¥åªæ”¯æŒå•ä¸ªå†™å…¥è€…ï¼Œå†™æ“ä½œæ€»æ˜¯ä»¥â€œåªæ·»åŠ â€æ–¹å¼åœ¨æ–‡ä»¶æœ«å°¾å†™å…¥æ•°æ®
```

### æ¦‚å¿µ
1. æ•°æ®å—
	```
		æ•°æ®å—ä¹Ÿç§°ä¸ºå­˜å‚¨å—ï¼Œæ˜¯ä¸»å­˜å‚¨å™¨ä¸è¾“å…¥ã€è¾“å‡ºè®¾å¤‡ä¹‹é—´è¿›è¡Œä¼ è¾“çš„æ•°æ®å•ä½ï¼Œæ˜¯ç£ç›˜è¿›è¡Œæ•°æ®è¯»/å†™çš„æœ€å°å•ä½ã€‚
		HDFSä¹Ÿæœ‰å—çš„æ¦‚å¿µï¼Œé»˜è®¤ä¸º128MBã€‚HDFSä¸Šçš„æ–‡ä»¶ä¹Ÿè¢«åˆ’åˆ†ä¸ºå—å¤§å°çš„å¤šä¸ªåˆ†å—ï¼ˆchunkï¼‰ï¼Œä½œä¸ºç‹¬ç«‹çš„å­˜å‚¨å•å…ƒã€‚æŠ½è±¡å‡ºè¿™æ ·çš„å—ä¼šå¸¦æ¥å¾ˆå¤šå¥½å¤„ï¼šä¸€ä¸ªæ–‡ä»¶çš„å¤§å°å¯ä»¥å¤§é›¨ç½‘ç»œä¸­ä»»æ„ä¸€ä¸ªç£ç›˜çš„å®¹é‡ï¼Œæ–‡ä»¶çš„æ‰€æœ‰å—å¯ä»¥åˆ©ç”¨é›†ç¾¤ä¸Šçš„ä»»æ„ä¸€ä¸ªç£ç›˜è¿›è¡Œå­˜å‚¨;å—é€‚ç”¨äºæ•°æ®å¤‡ä»½ï¼Œä»è€Œæé«˜æ•°æ®çš„å®¹é”™èƒ½åŠ›ï¼Œæé«˜å¯ç”¨æ€§ã€‚
	```
	
2. namenode å’Œ datenode
	```
		namenode(ç®¡ç†èŠ‚ç‚¹)ï¼šç®¡ç†æ–‡ä»¶ç³»ç»Ÿçš„å‘½åç©ºé—´ï¼Œç»´æŠ¤æ–‡ä»¶ç³»ç»Ÿæ ‘åŠæ•´æ£µæ ‘å†…æ‰€æœ‰çš„æ–‡ä»¶å’Œç›®å½•ï¼Œè¿™äº›ä¿¡æ¯ä»¥å‘½åç©ºé—´é•œåƒæ–‡ä»¶å’Œç¼–è¾‘æ—¥å¿—æ–‡ä»¶å½¢å¼ä¿å­˜åœ¨ç£ç›˜ã€‚æ­¤å¤–ï¼Œè¿˜è®°å½•ç€æ¯ä¸ªæ–‡ä»¶ä¸­å„ä¸ªå—æ‰€åœ¨çš„æ•°æ®èŠ‚ç‚¹ä¿¡æ¯ã€‚
		datenodeï¼ˆå·¥ä½œèŠ‚ç‚¹ï¼‰ï¼šæ ¹æ®éœ€è¦å­˜å‚¨å¹¶æ£€ç´¢æ•°æ®å—ï¼ˆå—å®¢æˆ·ç«¯/namenodeçš„è°ƒåº¦ï¼‰ï¼Œå¹¶å®šæœŸå‘namenodeæ±‡æŠ¥æ‰€å­˜å‚¨çš„å—çš„åˆ—è¡¨ã€‚
	```

### äº¤äº’æµç¨‹ç¤ºæ„å›¾
1. å®¢æˆ·ç«¯è¯»å–HDFSæ–‡ä»¶æµç¨‹
		![å®¢æˆ·ç«¯è¯»å–HDFSæµç¨‹](https://raw.githubusercontent.com/wudongsen/study/master/src/test/docImages/å®¢æˆ·ç«¯è¯»å–HDFSæµç¨‹.png)
	- æ­¥éª¤1:è°ƒç”¨FileSystemçš„open()æ‰“å¼€å¸Œæœ›è¯»å–çš„æ–‡ä»¶ã€‚
	- æ­¥éª¤2:DistributedFileSystemé€šè¿‡rpcè°ƒç”¨namenodeï¼Œç¡®å®šæ–‡ä»¶èµ·å§‹å—çš„ä½ç½®ã€‚å¯¹äºæ¯ä¸ªå—ï¼Œnamenodeè¿”å›å­˜æœ‰è¯¥å—å‰¯æœ¬çš„datanodeçš„åœ°å€ã€‚æ­¤å¤–ï¼Œdatanodeæ ¹æ®å®ƒä»¬ä¸å®¢æˆ·ç«¯çš„è·ç¦»æ¥æ’åºã€‚ç„¶åè¿”å›FSDataInputStreamç»™å®¢æˆ·ç«¯ã€‚
	- æ­¥éª¤3:FSDataInputStreamè°ƒç”¨read()ã€‚
	- æ­¥éª¤4:è¿æ¥è·ç¦»æœ€è¿‘çš„æ–‡ä»¶ä¸­çš„ç¬¬ä¸€ä¸ªå—æ‰€åœ¨çš„datanodeï¼Œé€šè¿‡å¯¹æ•°æ®æµåå¤è°ƒç”¨read()ï¼Œå°†æ•°æ®ä¼ è¾“åˆ°å®¢æˆ·ç«¯ã€‚
	- æ­¥éª¤5:è¾¾åˆ°å—çš„æœ«ç«¯æ—¶ï¼ŒDFSInputStreamå…³é—­ä¸è¯¥datanodeçš„è¿æ¥ï¼Œç„¶åå¯»æ‰¾ä¸‹ä¸€ä¸ªå—çš„æœ€ä½³datanodeã€‚
	- æ­¥éª¤6:å®¢æˆ·ç«¯å®Œæˆè¯»å–ï¼Œclose()ã€‚
		
2. å®¢æˆ·ç«¯å†™å…¥HDFSæµç¨‹å›¾
		![å®¢æˆ·ç«¯å†™å…¥HDFS](https://raw.githubusercontent.com/wudongsen/study/master/src/test/docImages/å®¢æˆ·ç«¯å°†æ•°æ®å†™å…¥HDFS.png)
	- æ­¥éª¤1:DistributedFileSystemè°ƒç”¨create()
	- æ­¥éª¤2:DistributedFileSystemå‘èµ·rpcè°ƒç”¨ï¼Œåœ¨ç¡®ä¿è¯¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ä¸”å®¢æˆ·ç«¯æœ‰æ–°å»ºè¯¥æ–‡ä»¶å¤¹çš„æƒé™çš„ä¸€ç³»åˆ—æ ¡éªŒåï¼Œnamenodeä¼šä¸ºåˆ›å»ºæ–°æ–‡ä»¶è®°å½•ä¸€å¤©è®°å½•ï¼Œå¹¶è¿”å›FSDataPutputStreamå¯¹è±¡ã€‚
	- æ­¥éª¤3:å®¢æˆ·ç«¯è°ƒç”¨write()ã€‚
	- æ­¥éª¤4ï¼šDFSOutputStreamå°†å®ƒåˆ†æˆä¸€ä¸ªä¸ªæ•°æ®åŒ…ï¼Œå†™å…¥æ•°æ®é˜Ÿåˆ—ã€‚DataStreameræŒ‘é€‰å‡ºé€‚åˆå­˜å‚¨æ•°æ®å‰¯æœ¬çš„ä¸€ç»„datanodeã€‚è¿™ä¸€ç»„datanodeæ„æˆä¸€ä¸ªç®¡çº¿ã€‚å‡è®¾å‰¯æœ¬æ•°ä¸º3ï¼ŒDataStreamerå°†æ•°æ®åŒ…æµå¼ä¾æ¬¡ä»ç¬¬ä¸€ä¸ªdatanodeä¼ è¾“åˆ°ç¬¬ä¸‰ä¸ªdatanodeã€‚
	- æ­¥éª¤5:DFSOutputStreamç»´æŠ¤ç€ç¡®è®¤é˜Ÿåˆ—ï¼Œç­‰æ”¶åˆ°ç®¡é“ä¸­æ‰€æœ‰datanodeçš„ç¡®è®¤ä¿¡æ¯åï¼Œæ•°æ®åŒ…æ‰ä¼šä»ç¡®è®¤é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
	- æ­¥éª¤6:å®Œæˆæ•°æ®å†™å…¥åï¼Œclose()ã€‚
	- æ­¥éª¤7:å‘ŠçŸ¥namenodeæ–‡ä»¶å†™å…¥å®Œæˆã€‚

### æ­å»º
ç³»ç»Ÿå’Œè½¯ä»¶ | ç‰ˆæœ¬å· | æ•°é‡
----------- | ------- | ---
MacBook Pro | 10.13.3 mac OS | 1å°
hadoop | 2.8.3 | 
jdk | 1.8.0_162

1. ç”¨æˆ·/ç›®å½•/äº‹å…ˆçº¦å®š
	* å·²å®‰è£…ã€é…ç½®å¥½jdk
	* ç”¨æˆ·ä½¿ç”¨hadoopuser
	* ä¸‹è½½æ–‡ä»¶åŒ…æ”¾ç½®äº/Users/hadoopuser/Downloads
	* hadoop_homeçš„ç›®å½•ä¸º/Users/hadoopuser/www/hadoop-2.8.3
2. ä¸‹è½½hadoop,è§£å‹åˆ°çº¦å®šç›®å½•
	 * brew install wget
	 * su hadoopuser
	 * cd /Users/hadoopuser/Downloads
	 * wget http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz
	 * tar zxvf hadoop-2.8.3.tar.gz -C /Users/hadoopuser/www/
3. ä¿®æ”¹é…ç½®æ–‡ä»¶
	* mkdir /Users/hadoopuser/www/hadoop-2.8.3/tmp
	* cd /Users/hadoopuser/www/hadoop-2.8.3/etc/hadoop
	* ä¿®æ”¹hadoop-env.sh

		```
		export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home
		export HADOOP_CONF_DIR=/Users/hadoopuser/www/hadoop-2.8.3/etc/hadoop
		```
	* ä¿®æ”¹core-site.xml

		```
		<property>
      	  <name>fs.defaultFS</name>
          <value>hdfs://localhost:9000</value>
        </property>

  	    <!--ç”¨æ¥æŒ‡å®šhadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜æ”¾ç›®å½•-->
  	    <property>
           <name>hadoop.tmp.dir</name>
           <value>/Users/hadoopuser/www/hadoop-2.8.3/tmp</value>
  		</property>	
		```
	* ä¿®æ”¹hdfs-site.xml

		```
		<property>
     		<!--ä¼ªåˆ†å¸ƒå¼æ¨¡å¼-->
     		<name>dfs.replication</name>
     		<value>1</value>
        </property>

 		<!--érootç”¨æˆ·ä¹Ÿå¯ä»¥å†™æ–‡ä»¶åˆ°hdfs-->
        <property>
        	<name>dfs.permissions</name>
     		<value>false</value>    
        </property>
		```
	* ä¿®æ”¹mapred-site.xml

		```
		<property>
    		<!--æŒ‡å®šmapreduceè¿è¡Œåœ¨yarnä¸Š-->
    		<name>mapreduce.framework.name</name>
    		<value>yarn</value>
        </property>
		```
	* ä¿®æ”¹yarn-site.xml

		```
		<!-- Site specific YARN configuration properties -->
        <property>
    			<name>yarn.resourcemanager.hostname</name>
    			<value>localhost</value>
        </property>
        <property>
    			<name>yarn.nodemanager.hostname</name>
    			<value>localhost</value>
  			</property>
        <property>
    			<name>yarn.nodemanager.aux-services</name>
    			<value>mapreduce_shuffle</value>
        </property>
		```
4. é…ç½®hadoopå…¨å±€ç¯å¢ƒå˜é‡
	* vim /etc/prifile
	* ä¿®æ”¹profile

		```
		export HADOOP_HOME=/Users/hadoopuser/www/hadoop-2.8.3
		export PATH=$PATH:$HADOOP_HOME/bin
		```
	* source /etc/profile
5. é…ç½®å…å¯†åŒ™ssh localhost
	* ssh-keygen -t rsa
	* cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	* cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	* ç”¨ssh localhostéªŒè¯ä¸‹
6. æ ¼å¼åŒ–nameNode
	* hadoop namenode -format
6. å¯åŠ¨
	* cd /Users/hadoopuser/www/hadoop-2.8.3/sbin
	* ./start-all.sh
7. éªŒè¯ 
	NameNode webç®¡ç†ç«¯å£ï¼šhttp://localhost:50070/

### shellå‘½ä»¤å­¦ä¹ 
åŸºæœ¬å‘½ä»¤æ ¼å¼ï¼šhadoop fs -cmd < args >

å‘½ä»¤è¡Œ | ä½œç”¨
------------------ | ----------------
hadoop fs -ls #path | åˆ—å‡ºè·¯å¾„æŒ‡å®šçš„ç›®å½•ä¸­çš„å†…å®¹
hadoop fs -lsr #path | é€’å½’æ˜¾ç¤ºè·¯å¾„çš„æ‰€æœ‰å­ç›®å½•é¡¹
hadoop fs -mkdir #path | hdfsä¸­åˆ›å»ºä¸€ä¸ªç›®å½•
hadoop fs -mv #src #dest | æŠŠæ–‡ä»¶/ç›®å½•ä»æºç§»åŠ¨åˆ°ç›®çš„
hadoop fs -cp #src #test | æŠŠæ–‡ä»¶/ç›®å½•ä»æºå¤åˆ¶åˆ°ç›®çš„
hadoop fs -rm #path | åˆ é™¤æ–‡ä»¶/ç›®å½•
hadoop fs -rmr #path | é€’å½’åˆ é™¤æ–‡ä»¶/ç›®å½•
hadoop fs -put #localSrc #dest | æœ¬åœ°æ–‡ä»¶å¤åˆ¶åˆ°hdfsçš„ç›®æ ‡æ–‡ä»¶
hadoop fs -get #src #localDest | æ‹·è´hdfsé‡Œçš„æ–‡ä»¶åˆ°æœ¬åœ°
hadoop fs -cat #fileName | æ˜¾ç¤ºæ–‡ä»¶å†…å®¹

---

## MapReduce
### ä¸¾ä¸ªæ —å­ğŸŒ°
1. example
```
ç»™å®šä¸€ä¸ªåç§°ä¸ºdata.txtçš„æ–‡æ¡£ï¼Œæƒ³è¦ç»Ÿè®¡å‡ºè¿™ä»½æ–‡æ¡£çš„æ¯ä¸ªå•è¯çš„æ•°é‡ã€‚æ–‡æ¡£æ•°æ®å†…å®¹ä¸ºï¼š  
    tom animal
    wds man
    peiqi animal
é‚£ä¹ˆè¿ç”¨MapReduceå¯¹å…¶è¿›è¡Œå®ç°çš„è¿‡ç¨‹ä¸ºï¼š
    1. input:è¾“å…¥è¯¥æ–‡æ¡£
        tom animal
        wds man
        peiqi animal
    2. split
        split-0:tom animal
        split-0:wds man
        split-1:peiqi animal
    3. map:å°†å†…å®¹è½¬æ¢ä¸ºæ‰€éœ€è¦çš„key value
        split-0:
            tom 1
            animali 1
            wds 1
            man 1
        split-1:
            peiqi 1
            animal 1
    4. shuffle: å°†keyç›¸åŒçš„æ´¾å‘åˆ°ä¸€èµ·
        tom:1
        animali:1,1
        wds:1
        man:1
        peiqi:1
    5.reduce:å°†ç›¸åŒkeyç»“æœè¿›è¡Œç»Ÿè®¡
        tom:1
        animali:2
        wds:1
        man:1
        peiqi:1
    6.output
```
2. é…å›¾  
    ![MapReduce-wordCountç¤ºæ„å›¾.png](https://raw.githubusercontent.com/wudongsen/study/master/src/test/docImages/MapReduce-wordCountç¤ºæ„å›¾.png)

---

## yarn

## hive
### è¿œç¨‹æ¨¡å¼æ­å»º
ç³»ç»Ÿå’Œè½¯ä»¶ | ç‰ˆæœ¬å· | æ•°é‡
----------- | ------- | ---
MacBook Pro | 10.13.3 mac OS | 1å°
hadoop | 2.8.3 | 
jdk | 1.8.0_162 | 
mysql | 5.6.39 | 
hive | 2.3.3 | 

1. ç”¨æˆ·/ç›®å½•/äº‹å…ˆçº¦å®š
	* å·²å®‰è£…å¥½jdkã€mysqlã€hadoopç¯å¢ƒ
	* ç”¨æˆ·ä½¿ç”¨hadoopuser
	* ä¸‹è½½æ–‡ä»¶åŒ…æ”¾ç½®äº/Users/hadoopuser/Downloads
	* hive_homeçš„ç›®å½•ä¸º/Users/hadoopuser/www/hive-2.3.3
2. ä¸‹è½½hiveã€mysqlé©±åŠ¨åŒ…,è§£å‹åˆ°çº¦å®šç›®å½•,å¹¶æŠŠmysqlé©±åŠ¨åŒ…æ·»åŠ åˆ°hiveçš„libç›®å½•ä¸‹
	* su hadoopuser
	* cd /Users/hadoopuser/Downloads
	* wget http://mirrors.hust.edu.cn/apache/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz
	* wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz
	* tar zxvf apache-hive-2.3.3-bin.tar.gz -C /Users/hadoopuser/www/
	* tar zxvf mysql-connector-java-5.1.46.tar
	* cd mysql-connector-java-5.1.46
	* cp mysql-connector-java-5.1.46-bin.jar /Users/hadoopuser/www/hive-2.3.3/lib/
3. æ›´æ”¹hive-site.xmlé…ç½®æ–‡ä»¶
	* mkdir mkdir /Users/hadoopuser/www/hive-2.3.3/tempï¼ˆåˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜æ”¾è·¯å¾„ï¼‰
	* cd /Users/hadoopuser/www/hive-2.3.3/conf
	* cp hive-default.xml.template hive-site.xml
	* vim hive-site.xml
		* æŠŠ${system:java.io.tmpdir}å…¨éƒ¨æ›¿æ¢æˆ/Users/hadoopuser/www/hive-2.3.3/temp
		* æŠŠ{system:user.name}å…¨éƒ¨æ¢æˆ{user.name}
	* é…ç½®æ•°æ®åº“

		```
			<name>javax.jdo.option.ConnectionURL</name>
    		<value>jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true</value>
    		
    		<name>javax.jdo.option.ConnectionDriverName</name>
			<value>com.mysql.jdbc.Driver</value>
			
			<name>javax.jdo.option.ConnectionUserName</name>
 			<value>hive</value>
 			
			<name>javax.jdo.option.ConnectionPassword</name>
			<value>hive</value>
		```			
4. åˆ›å»ºæ•°æ®åº“ç”¨æˆ·ã€æ•°æ®åº“
	* /usr/local/mysql/bin
	* ./mysql -h127.0.0.1 -P3306 -uroot -p
	* create database hive;
	* grant all on hive.* to hive@'%'  identified by 'hive';
	* grant all on hive.* to hive@'localhost'  identified by 'hive';
	* flush privileges;
5. åˆå§‹åŒ–metadata
	* ./schematool -initSchema -dbType mysql --verbose
	* åœ¨mysqlä¸ŠæŸ¥çœ‹æ˜¯å¦æˆåŠŸ
6. å¯åŠ¨hive
	* ./hive --service hiveserver2 &

### hiveå‘½ä»¤
1. DDL(data definition language)æ“ä½œ
	* å»ºè¡¨  
  CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name  
  [(col_name data_type [COMMENT col_comment], ...)]  
  [COMMENT table_comment]  
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]   
  [CLUSTERED BY (col_name, col_name, ...)   
  [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  
  [ROW FORMAT row_format]  
  [STORED AS file_format]  
  [LOCATION hdfs_path]
  		- EXTERNALï¼šå¯ä»¥è®©ç”¨æˆ·åˆ›å»ºä¸€ä¸ªå¤–éƒ¨è¡¨ï¼Œåœ¨å»ºè¡¨çš„åŒæ—¶æŒ‡å®šä¸€ä¸ªæŒ‡å‘å®é™…æ•°æ®çš„è·¯å¾„ï¼ˆLOCATIONï¼‰
  		- ROW FORMATï¼š
  			
  			DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
  		- PARTITIONED BYï¼šæŒ‡å®šåˆ†åŒºä¿¡æ¯
  		- CLUSTERED BYï¼šé’ˆå¯¹æŸä¸€åˆ—åˆ†æ¡¶ï¼Œé‡‡ç”¨å¯¹åˆ—å€¼è¿›è¡Œå“ˆå¸Œï¼Œç„¶åé™¤ä»¥æ¡¶çš„ä¸ªæ•°æ±‚ä½™çš„æ–¹å¼å†³å®šè¯¥æ¡è®°å½•å­˜æ”¾åœ¨å“ªä¸ªæ¡¶å½“ä¸­ã€‚æ›´ç»†ç²’åº¦åœ°åˆ’åˆ†æ•°æ®ã€‚
  		- STORED AS SEQUENCEFILE | TEXTFILE| RCFILEã€‚å¦‚æœæ–‡ä»¶æ•°æ®æ˜¯çº¯æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨TEXTFILEã€‚å¦‚æœæ•°æ®éœ€è¦å‹ç¼©ï¼Œä½¿ç”¨SEQUENCEFILEã€‚
  	caseï¼š
  	```
  		CREATE TABLE par(userid BIGINT, time INT, page_url STRING,referrer_url STRING, ip STRING COMMENT 'IP Address of the User')  
  		COMMENT 'This is the page view table'  
  		PARTITIONED BY(dt STRING) CLUSTERED BY(userid)   
  		SORTED BY(time) INTO 32 BUCKETS   
  		ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '  LINES TERMINATED BY '\n' STORED AS TEXTFILE;
  	```
	* æŸ¥çœ‹æ•°æ®åº“
	    ```
	    show databases;
	    ```
	* æŸ¥çœ‹å…·ä½“æ•°æ®åº“
	    ```
		describe database default;
		```
	* æŸ¥çœ‹åˆ›å»ºæ•°æ®åº“sqlè¯­å¥
	    ```
		show create database default;
		```
	* æŸ¥çœ‹è¡¨è¯¦ç»†ä¿¡æ¯
	    ```
		desc par;
        describe extended par;
 		describe formatted par;
 		```
	* åˆ é™¤è¡¨
	    ```
		drop table par;
		```
	* æ¸…ç©ºè¡¨æ•°æ®
	    ```
		truncate table emp_copy;
		```
2. DML(data mainpulation language)æ“ä½œ
	* åŠ è½½æœ¬åœ°æ–‡ä»¶
	    ```
		load data local inpath  
		'/Users/hadoopuser/www/hive-2.3.3/bin/1.txt' into table  
		default.par PARTITION (dt='2008-08-15');
		```
	* insert
	    ```
	    insert into test values(1,'22',3);
	   ```
	*
3. DQL(data query language)æ“ä½œ
## å‚è€ƒæ–‡çŒ®
> [1]Tom White.Hadoopæƒå¨æŒ‡å—[M].åŒ—äº¬:æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾,ç¬¬å››ç‰ˆ.
